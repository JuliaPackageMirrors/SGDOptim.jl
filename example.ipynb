{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a ridge regression problem as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{minimize } \\ \\frac{1}{2} \\sum_{i=1}^n (\\theta^T x_i - y_i)^2 + \\frac{\\lambda}{2} \\|\\theta\\|^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem is a special case of a more general family of problems called *regularized empirical risk minimization*, where the objective function is usually comprised of two parts: a set of *loss terms* and a *regularization term*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we show how to use the package *SGDOptim* to solve such a problem. First, we have to prepare some simulation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       "  3.0\n",
       " -4.0\n",
       "  5.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "θ = [3.0, -4.0, 5.0];   # the underlying model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3x10000 Array{Float64,2}:\n",
       "  0.186357   2.06501  -1.30345    …  -0.213211  -1.28663   -0.13952 \n",
       " -1.11419   -1.76544   0.0552147     -0.16457   -2.1969     1.49355 \n",
       "  1.04388   -2.16657  -1.17215        0.271101   0.781125  -0.242643"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10000; X = randn(3, n);  # generate 10000 sample features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000-element Array{Float64,1}:\n",
       "  10.109   \n",
       "   2.34337 \n",
       "  -9.96466 \n",
       "  -4.55535 \n",
       "  -1.01875 \n",
       "  14.1827  \n",
       " -14.0844  \n",
       "   7.13437 \n",
       " -13.5125  \n",
       "  14.5484  \n",
       "  14.439   \n",
       "   7.56407 \n",
       "   0.982114\n",
       "   ⋮       \n",
       "   5.33694 \n",
       "  -7.16893 \n",
       "   8.59394 \n",
       "  -1.94164 \n",
       "   7.21264 \n",
       "   9.72875 \n",
       "   4.03635 \n",
       "  -3.00291 \n",
       "  16.838   \n",
       "   1.37921 \n",
       "   8.82616 \n",
       "  -7.80379 "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "σ = 0.1; y = vec(θ'X) + σ * randn(n); # generate the responses, adding some noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to estimate $\\theta$ from the data. This can be done by the following statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using SGDOptim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100: avg.loss = 2.5217e-03\n",
      "Iter 200: avg.loss = 5.6985e-03\n",
      "Iter 300: avg.loss = 2.7370e-03\n",
      "Iter 400: avg.loss = 9.2170e-03\n",
      "Iter 500: avg.loss = 5.7844e-03\n",
      "Iter 600: avg.loss = 4.9012e-03\n",
      "Iter 700: avg.loss = 8.3151e-03\n",
      "Iter 800: avg.loss = 3.8253e-03\n",
      "Iter 900: avg.loss = 3.1001e-03\n",
      "Iter 1000: avg.loss = 6.1005e-03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       "  2.99657\n",
       " -4.00036\n",
       "  4.99567"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "θe = sgd(\n",
    "    LinearPredictor(),   # use the linear predictor x -> θ'x\n",
    "    SqrLoss(),     # use the squared loss (the typical choice for linear regression)\n",
    "    zeros(3),      # the initial guess\n",
    "    minibatch_seq(X, y, 10),    # supply the data in mini-batches, each with 10 samples\n",
    "    reg = SqrL2Reg(1.0e-4),     # add a squared L2 regression with coefficient 1.0e-4\n",
    "    lrate = t->1.0/(100.0 + t), # set the rule of learning rate \n",
    "    cbinterval = 100,           # invoke the callback every 100 iterations\n",
    "    callback = simple_trace)    # print the optimization trace in callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that 10000 samples can be partitioned into 1000 minibatches of size 10. So there were 1000 iterations, each using a single minibatch.\n",
    "\n",
    "Now let's compare the estimated solution with the ground-truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.133004112320621e-7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumabs2(θe - θ) / sumabs2(θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result looks quite accurate. We are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0-dev",
   "language": "julia",
   "name": "julia 0.4"
  },
  "language_info": {
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
